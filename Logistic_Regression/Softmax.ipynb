{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe6e6bfb-64f5-4d7f-bf75-9872b2d417ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c10c9c6-96fb-43c4-9af0-c71e56ebd802",
   "metadata": {},
   "source": [
    "# Softmax function for multi-class classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fca0e070-c36c-4db6-865c-2c6917966943",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))  # For numerical stability\n",
    "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d729020-083b-461d-ad3b-594d37537fe0",
   "metadata": {},
   "source": [
    "## Cross-entropy loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3146a1e-1204-418a-be5c-41aa7c9472bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(Y, Y_hat):\n",
    "    m = Y.shape[0]\n",
    "    return -np.sum(Y * np.log(Y_hat)) / m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09000f8-c9e1-417a-9a7b-3fb1d1fb8732",
   "metadata": {},
   "source": [
    "## Gradient of the loss function with respect to weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae696e14-b7d3-4738-9f2b-612d96e5a599",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradients(X, Y, Y_hat):\n",
    "    m = X.shape[0]\n",
    "    dW = np.dot(X.T, (Y_hat - Y)) / m\n",
    "    dB = np.sum(Y_hat - Y, axis=0, keepdims=True) / m\n",
    "    return dW, dB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318fa3a5-e467-426d-8497-1d3b5c708ae8",
   "metadata": {},
   "source": [
    "## Training the multinomial logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29c526de-a117-443f-87ea-8a46570bb91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_multinomial_logistic_regression(X, Y, num_classes, learning_rate=0.01, num_epochs=1000):\n",
    "    num_samples, num_features = X.shape\n",
    "    weights = np.random.randn(num_features, num_classes) * 0.01  # Small random initialization\n",
    "    bias = np.zeros((1, num_classes))\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Forward pass\n",
    "        Z = np.dot(X, weights) + bias  # Linear combination\n",
    "        Y_hat = softmax(Z)  # Predicted probabilities\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = cross_entropy_loss(Y, Y_hat)\n",
    "\n",
    "        # Backward pass (gradient calculation)\n",
    "        dW, dB = compute_gradients(X, Y, Y_hat)\n",
    "        \n",
    "        # Gradient descent step\n",
    "        weights -= learning_rate * dW\n",
    "        bias -= learning_rate * dB\n",
    "        \n",
    "        if epoch % 100 == 0:\n",
    "            print(f'Epoch {epoch}/{num_epochs}, Loss: {loss:.4f}')\n",
    "\n",
    "    return weights, bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967ae6a9-45f1-4172-959f-ec70b8022efb",
   "metadata": {},
   "source": [
    "## Prediction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74a0112f-bfc9-4856-9cde-c2f80c953a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, weights, bias):\n",
    "    Z = np.dot(X, weights) + bias\n",
    "    Y_hat = softmax(Z)\n",
    "    return np.argmax(Y_hat, axis=1)  # Return the class with the highest probability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cede8a3-3ba4-48b5-b8c6-390f3ffc57e3",
   "metadata": {},
   "source": [
    "### Generating dummy data: 100 samples, 3 features, 3 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f2ecff1-129a-45cc-9deb-4b33bb409034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/300, Loss: 1.0982\n",
      "Epoch 100/300, Loss: 1.0546\n",
      "Epoch 200/300, Loss: 1.0539\n",
      "Predictions: [0 0 1 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "X_train = np.random.randn(100, 3)  # 100 samples, 3 features\n",
    "y_train = np.random.randint(0, 3, 100)  # 100 labels for 3 classes\n",
    "\n",
    "# One-hot encode the labels\n",
    "Y_train = np.eye(3)[y_train]  # Convert to one-hot encoded labels\n",
    "\n",
    "# Train the model\n",
    "weights, bias = train_multinomial_logistic_regression(X_train, Y_train, num_classes=3, learning_rate=0.1, num_epochs=300)\n",
    "\n",
    "# Make predictions\n",
    "predictions = predict(X_train, weights, bias)\n",
    "print(f\"Predictions: {predictions[:10]}\")  # Print first 10 predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
